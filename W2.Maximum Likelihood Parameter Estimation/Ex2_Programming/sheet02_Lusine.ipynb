{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 2: Maximum Likelihood Estimation\n",
    "\n",
    "In this exercise sheet, we will look at various properties of maximum-likelihood estimation, and how to find maximum-likelihood parameters.\n",
    "\n",
    "### ML vs. James Stein Estimator (15 P)\n",
    "\n",
    "Let $X_1,\\dots,X_n \\in \\mathbb{R}^d$ be independent draws from a multivariate Gaussian distribution with mean vector $\\mu$ and covariance matrix $\\Sigma = \\sigma^2 I$. It can be shown that the maximum-likelihood estimator of the mean parameter $\\mu$ is the empirical mean given by:\n",
    "$$\n",
    "\\hat \\mu_\\text{ML} = \\frac1N \\sum_{i=1}^N X_i\n",
    "$$\n",
    "It was once believed that the maximum-likelihood estimator was the most accurate possible (i.e. the one with the smallest Euclidean distance from the true mean). However, it was later demonstrated that the following estimator\n",
    "$$\n",
    "\\hat \\mu_{JS} = \\Big(1-\\frac{(d-2) \\cdot \\sigma^2}{n \\cdot \\|\\mu_\\text{ML}\\|^2}\\Big) \\hat \\mu_\\text{ML}\n",
    "$$\n",
    "(a shrinked version of the maximum-likelihood estimator towards the origin) has actually a smaller distance from the true mean when $d \\geq 3$. This however assumes knowledge of the variance of the distribution for which the mean is estimated. This estimator is called the James-Stein estimator. While the proof is a bit involved, this fact can be easily demonstrated empirically through simulation. This is the object of this exercise.\n",
    "\n",
    "The code below draws ten 50-dimensional points from a normal distribution with mean vector $\\mu = (1,\\dots,1)$ and covariance $\\Sigma = I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "def getdata(seed):\n",
    "\n",
    "    n = 10              # data points\n",
    "    d = 50              # dimensionality of data\n",
    "    m = numpy.ones([d]) # true mean\n",
    "    s = 1.0             # true standard deviation\n",
    "\n",
    "    rstate = numpy.random.mtrand.RandomState(seed)\n",
    "    X = rstate.normal(0,1,[n,d])*s+m\n",
    "    return X,m,s\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the maximum likelihood estimator from a sample of the data assumed to be generated by a Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def ML(X):\n",
    "    return X.mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Based on the ML estimator function, write a function that receives as input the data $(X_i)_{i=1}^n$ and the (known) variance $\\sigma^2$ of the generating distribution, and computes the James-Stein estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def JS(X,s):\n",
    "    n = len(X)\n",
    "    d = 50\n",
    "    m_JS = (1 - ((d -2)*s**2)/(n*(numpy.linalg.norm(X)**2)))*ML(X)\n",
    "    return m_JS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20848832984\n",
      "2.19871484322\n"
     ]
    }
   ],
   "source": [
    "def MLerror(X, m):\n",
    "    return numpy.sqrt(sum(((ML(X) - m)**2)))\n",
    "def JSerror(X, m):\n",
    "    return numpy.sqrt(sum(((JS(X, s) - m)**2)))\n",
    "MLmeanEr = 0\n",
    "JSmeanEr = 0\n",
    "\n",
    "for i in range(0,99):\n",
    "        X, m, s =  getdata(i)\n",
    "        n = len(range(0,99))+1\n",
    "        MLmeanEr = (MLmeanEr + MLerror(X, m))\n",
    "        JSmeanEr = (JSmeanEr + JSerror(X, m))\n",
    "        \n",
    "print MLmeanEr/n\n",
    "print JSmeanEr/n        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x116bb61d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLlist = []\n",
    "JSlist = []\n",
    "MLmeanEr = 0\n",
    "JSmeanEr = 0\n",
    "for i in range(0,99):\n",
    "        X, m, s =  getdata(i)\n",
    "        n = len(range(0,99))+1\n",
    "        MLlist.append(MLerror(X, m))\n",
    "        JSlist.append(JSerror(X, m))\n",
    "#print MLlist\n",
    "#print JSlist\n",
    "\n",
    "#error = numpy.matrix(MLlist)- numpy.matrix(JSlist)\n",
    "#print error\n",
    "\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(MLlist, JSlist, \"o\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119f6dd90>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to compute the error of the maximum likelihood estimator and the James-Stein estimator for 100 different samples (where each sample consists of 10 draws generated by the function `getdata` with a different random seed). Here, for reproducibility, we use seeds from 0 to 99. The error should be measured as the Euclidean distance between the true mean vector and the estimated mean vector.\n",
    "\n",
    "* **Compute the maximum-likelihood and James-Stein estimations.**\n",
    "* **Measure the error of these estimations.**\n",
    "* **Build a scatter plot comparing these errors for different samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "### REPLACE BY YOUR CODE\n",
    "import solution\n",
    "solution.compare_ML_JS()\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of a mixture of exponentials (15 P)\n",
    "\n",
    "We consider the following \"mixture of exponentials\" distribution supported on $\\mathbb{R}^+$, that we use to generate data, but whose parameters $\\alpha$ and $\\beta$ are unknown.\n",
    "\n",
    "$$p(x;\\alpha,\\beta) = 0.5 \\cdot \\big[\\alpha e^{-\\alpha x} + \\beta e^{-\\beta x}\\big]$$\n",
    "\n",
    "A dataset $\\mathcal{D} = x_1,\\dots,x_N$ with $N=200$ has been generated from that distribution. It is given below and plotted as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFkCAYAAACw3EhvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAGexJREFUeJzt3X+QXeV93/H3VxagCFeSJ0oku7YaGMWKPM7gaglEpdhN\nRU1FGwd3Oo1vUInDYIptZRg1bTBTeaKgNuPIY0uxg2tmmsTYmOtR2mQwhFjBkDT8iEVhZRibtTqK\nJctCaPGasKolC2Hr2z/Os87uIpY994fuvdr3a+YO3Oc855zvHJDu5z7nuc+JzESSJGlerwuQJEn9\nwVAgSZIAQ4EkSSoMBZIkCTAUSJKkwlAgSZIAQ4EkSSoMBZIkCTAUSJKkwlAgSZKAmqEgIvZHxKnT\nvD45qc+tEXE4Io5HxP0RsbLzZUuSpE6rO1JwMbB80utfAAnsBIiIm4GNwA3AJcAxYFdEnNupgiVJ\nUndEOw9EiogdwFWZ+eby/jDw0czcXt4vAkaBX83MnR2oV5IkdUnLcwoi4hzgGuAPyvsLqEYPHpjo\nk5lHgd3A2vbKlCRJ3Ta/jX3fDSwG7ijvl1PdShid1m+0bDutiPhx4ErgAHCijXokSZprFgA/BezK\nzO+2e7B2QsF1wJ9n5pE2a7gS+Hybx5AkaS67Brir3YO0FAoiYgVwBXD1pOYjQADLmDpasAzYM8Ph\nDgDceeedrF69upVy5qxNmzaxffv2XpcxULxmrfG61ec1a43XrZ6RkRE2bNgA5bO0Xa2OFFxH9cF/\n30RDZu6PiCPAOuAp+NFEw0uB22Y41gmA1atXs2bNmhbLmZsWL17sNavJa9Yar1t9XrPWeN1a1pHb\n77VDQUQE8F7gM5l5atrmHcDmiNhHlVq2AoeAu9srU5IkdVsrIwVXAG8C/mj6hszcFhELgduBJcBD\nwPrMPNlWlZIkqetqh4LMvB94zQzbtwBbWi9JkiT1gs8+GGCNRqPXJQwcr1lrvG71ec1a43XrrbZW\nNOxIARFrgCeeeOIJJ5dIklTD8PAwQ0NDAEOZOdzu8RwpkCRJgKFAkiQVhgJJkgQYCiRJUmEokCRJ\ngKFAkiQVhgJJkgQYCiRJUmEokCRJgKFAkiQVhgJJkgQYCiRJUmEokCRJAMzvdQETnnvuOZ555plZ\n93/d617HwoULu1iRJElzS9+EgvXr19fq/5a3vI2vf31Pl6qRJGnu6ZtQAJ8EVs6y75+xd++nu1mM\nJElzTh+Fgn8CrJll3wNdrEOSpLnJiYaSJAkwFEiSpMJQIEmSAEOBJEkqDAWSJAkwFEiSpMJQIEmS\nAEOBJEkqDAWSJAkwFEiSpMJQIEmSAEOBJEkqDAWSJAkwFEiSpKJ2KIiIN0TE5yJiLCKOR8STEbFm\nWp9bI+Jw2X5/RKzsXMmSJKkbaoWCiFgCPAK8CFwJrAZ+A/i7SX1uBjYCNwCXAMeAXRFxbodqliRJ\nXTC/Zv8PAQcz8/pJbd+a1ucmYGtm3gsQEdcCo8DVwM5WC5UkSd1V9/bBLwKPR8TOiBiNiOGI+FFA\niIgLgOXAAxNtmXkU2A2s7UTBkiSpO+qGgguB9wN7gXcC/x34RET8+7J9OZBUIwOTjZZtkiSpT9W9\nfTAPeCwzP1zePxkRbwVuBD7X0cokSdIZVTcUPAuMTGsbAf5N+fcjQADLmDpasAzYM/OhNwGLp7U1\nykuSpLmt2WzSbDantI2Pj3f0HHVDwSPAqmltqyiTDTNzf0QcAdYBTwFExCLgUuC2mQ+9HVgzcxdJ\nkuaoRqNBozH1i/Lw8DBDQ0MdO0fdULAdeCQibqH6JcGlwPXA+yb12QFsjoh9wAFgK3AIuLvtaiVJ\nUtfUCgWZ+XhEvBv4CPBhYD9wU2Z+YVKfbRGxELgdWAI8BKzPzJOdK1uSJHVa3ZECMvM+4L5X6bMF\n2NJaSZIkqRd89oEkSQIMBZIkqTAUSJIkwFAgSZIKQ4EkSQIMBZIkqTAUSJIkwFAgSZIKQ4EkSQIM\nBZIkqTAUSJIkwFAgSZIKQ4EkSQIMBZIkqTAUSJIkwFAgSZIKQ4EkSQIMBZIkqTAUSJIkwFAgSZIK\nQ4EkSQIMBZIkqTAUSJIkwFAgSZIKQ4EkSQIMBZIkqTAUSJIkwFAgSZIKQ4EkSQIMBZIkqTAUSJIk\nwFAgSZIKQ4EkSQIMBZIkqagVCiLityLi1LTX09P63BoRhyPieETcHxErO1uyJEnqhlZGCr4GLAOW\nl9c/ndgQETcDG4EbgEuAY8CuiDi3/VIlSVI3zW9hnx9k5ndeYdtNwNbMvBcgIq4FRoGrgZ2tlShJ\nks6EVkYKfjoinomIv42IOyPiTQARcQHVyMEDEx0z8yiwG1jbkWolSVLX1A0FXwHeC1wJ3AhcAPx1\nRJxPFQiSamRgstGyTZIk9bFatw8yc9ekt1+LiMeAbwH/DvhGe6VsAhZPa2uUlyRJc1uz2aTZbE5p\nGx8f7+g5WplT8COZOR4R/xdYCfwVEFSTECePFiwD9rz60bYDa9opR5Kks1aj0aDRmPpFeXh4mKGh\noY6do611CiLitVSB4HBm7geOAOsmbV8EXAo82s55JElS99UaKYiIjwL3UN0y+IfAbwMvAV8oXXYA\nmyNiH3AA2AocAu7uUL2SJKlL6t4+eCNwF/DjwHeAh4Gfz8zvAmTmtohYCNwOLAEeAtZn5snOlSxJ\nkrqh7kTDV531l5lbgC0t1iNJknrEZx9IkiTAUCBJkgpDgSRJAgwFkiSpMBRIkiTAUCBJkgpDgSRJ\nAgwFkiSpMBRIkiTAUCBJkgpDgSRJAgwFkiSpMBRIkiTAUCBJkgpDgSRJAgwFkiSpMBRIkiTAUCBJ\nkgpDgSRJAgwFkiSpmN/rAlqVmQwPD9faZ+nSpaxYsaJLFUmSNNgGNBQ8z6lTydDQUK29FixYyN69\nIwYDSZJOY0BDwfeAU8CdwOpZ7jPCiRMbGBsbMxRIknQaAxoKJqwG1vS6CEmSzgpONJQkSYChQJIk\nFYYCSZIEGAokSVJhKJAkSYChQJIkFYYCSZIEGAokSVJhKJAkSUCboSAiPhQRpyLi49Pab42IwxFx\nPCLuj4iV7ZUpSZK6reVQEBE/B9wAPDmt/WZgY9l2CXAM2BUR57ZRpyRJ6rKWQkFEvJbqaUTXAy9M\n23wTsDUz783MrwHXAm8Arm6nUEmS1F2tjhTcBtyTmQ9OboyIC4DlwAMTbZl5FNgNrG21SEmS1H21\nn5IYEe8B3gZcfJrNy4EERqe1j5ZtkiSpT9UKBRHxRmAHcEVmvtTZUjYBi6e1NcpLkqS5rdls0mw2\np7SNj4939Bx1RwqGgJ8AhiMiSttrgLdHxEbgZ4AAljF1tGAZsGfmQ28H1tQsR5KkuaHRaNBoTP2i\nPDw8zNDQUMfOUXdOwZeBn6W6fXBReT1ONenwosz8JnAEWDexQ0QsAi4FHu1EwZIkqTtqjRRk5jHg\n6cltEXEM+G5mjpSmHcDmiNgHHAC2AoeAu9uuVpIkdU3tiYankVPeZG6LiIXA7cAS4CFgfWae7MC5\nJElSl7QdCjLzn5+mbQuwpd1jS5KkM8dnH0iSJMBQIEmSCkOBJEkCDAWSJKkwFEiSJMBQIEmSCkOB\nJEkCDAWSJKkwFEiSJMBQIEmSCkOBJEkCDAWSJKkwFEiSJMBQIEmSCkOBJEkCDAWSJKkwFEiSJMBQ\nIEmSCkOBJEkCDAWSJKkwFEiSJMBQIEmSCkOBJEkCDAWSJKkwFEiSJMBQIEmSCkOBJEkCDAWSJKkw\nFEiSJMBQIEmSCkOBJEkCDAWSJKkwFEiSJKBmKIiIGyPiyYgYL69HI+JfTutza0QcjojjEXF/RKzs\nbMmSJKkb6o4UfBu4GVgDDAEPAndHxGqAiLgZ2AjcAFwCHAN2RcS5HatYkiR1Ra1QkJl/lplfysy/\nzcx9mbkZ+B7w86XLTcDWzLw3M78GXAu8Abi6o1VLkqSOa3lOQUTMi4j3AAuBRyPiAmA58MBEn8w8\nCuwG1rZbqCRJ6q75dXeIiLcCfwMsAP4f8O7M3BsRa4EERqftMkoVFiRJUh+rHQqAbwAXAYuBfwt8\nNiLe3tGqJEnSGVc7FGTmD4Bvlrd7IuISqrkE24AAljF1tGAZsOfVj7yJKmdM1igvSZLmtmazSbPZ\nnNI2Pj7e0XO0MlIw3TzgvMzcHxFHgHXAUwARsQi4FLjt1Q+znepHDZIkabpGo0GjMfWL8vDwMEND\nQx07R61QEBG/A/w5cBD4B8A1wDuAd5YuO4DNEbEPOABsBQ4Bd3eoXkmS1CV1Rwp+ErgDeD0wTjUi\n8M7MfBAgM7dFxELgdmAJ8BCwPjNPdq7k9oyMjNTqv3TpUlasWNGlaiRJ6h+1QkFmXj+LPluALS3W\n00XPAvPYsGFDrb0WLFjI3r0jBgNJ0lmvE3MKBsQLwCngTmD1LPcZ4cSJDYyNjRkKJElnvTkUCias\nxgmNkiS9nE9JlCRJgKFAkiQVhgJJkgQYCiRJUmEokCRJgKFAkiQVhgJJkgQYCiRJUmEokCRJgKFA\nkiQVhgJJkgQYCiRJUmEokCRJgKFAkiQVhgJJkgQYCiRJUmEokCRJgKFAkiQVhgJJkgQYCiRJUmEo\nkCRJgKFAkiQVhgJJkgQYCiRJUmEokCRJgKFAkiQVhgJJkgQYCiRJUmEokCRJgKFAkiQVhgJJkgTU\nDAURcUtEPBYRRyNiNCL+NCLefJp+t0bE4Yg4HhH3R8TKzpUsSZK6oe5IweXAJ4FLgSuAc4C/iIgf\nm+gQETcDG4EbgEuAY8CuiDi3IxVLkqSumF+nc2ZeNfl9RLwXeA4YAh4uzTcBWzPz3tLnWmAUuBrY\n2Wa9kiSpS9qdU7AESOB5gIi4AFgOPDDRITOPAruBtW2eS5IkdVHLoSAiAtgBPJyZT5fm5VQhYXRa\n99GyTZIk9alatw+m+RTwFuCyDtUiSZJ6qKVQEBG/D1wFXJ6Zz07adAQIYBlTRwuWAXtmPuomYPG0\ntkZ5SZI0tzWbTZrN5pS28fHxjp6jdigogeCXgHdk5sHJ2zJzf0QcAdYBT5X+i6h+rXDbzEfeDqyp\nW44kSXNCo9Gg0Zj6RXl4eJihoaGOnaNWKIiIT1F9dX8XcCwilpVN45l5ovz7DmBzROwDDgBbgUPA\n3R2pWJIkdUXdkYIbqSYS/tW09l8DPguQmdsiYiFwO9WvEx4C1mfmyfZKlSRJ3VR3nYJZ/VohM7cA\nW1qoR5Ik9YjPPpAkSYChQJIkFYYCSZIEGAokSVJhKJAkSYChQJIkFYYCSZIEGAokSVJhKJAkSYCh\nQJIkFYYCSZIEGAokSVJhKJAkSYChQJIkFYYCSZIEGAokSVJhKJAkSYChQJIkFYYCSZIEGAokSVJh\nKJAkSYChQJIkFYYCSZIEGAokSVJhKJAkSYChQJIkFYYCSZIEGAokSVJhKJAkSYChQJIkFYYCSZIE\nGAokSVJhKJAkSYChQJIkFfPr7hARlwP/GRgCXg9cnZlfnNbnVuB6YAnwCPD+zNzXfrm9MTIyUqv/\n0qVLWbFiRZeqkSSpO2qHAuB84KvAHwB/Mn1jRNwMbASuBQ4A/xXYFRGrM/Nk66X2wrPAPDZs2FBr\nrwULFrJ374jBQJI0UGqHgsz8EvAlgIiI03S5CdiamfeWPtcCo8DVwM7WS+2FF4BTwJ3A6lnuM8KJ\nExsYGxszFEiSBkorIwWvKCIuAJYDD0y0ZebRiNgNrGXgQsGE1cCaXhchSVJXdXqi4XIgqUYGJhst\n2yRJUp/q6EhBezYBi6e1NcpLkqS5rdls0mw2p7SNj4939BydDgVHgACWMXW0YBmwZ+Zdt+MQvSRJ\np9doNGg0pn5RHh4eZmhoqGPn6Ojtg8zcTxUM1k20RcQi4FLg0U6eS5IkdVYr6xScD6ykGhEAuDAi\nLgKez8xvAzuAzRGxj+oniVuBQ8DdHalYkiR1RSu3Dy4G/pJqQmECHyvtdwDXZea2iFgI3E61eNFD\nwPrBW6Og/x08eJCxsbFa+7iwkiTplbSyTsH/5lVuO2TmFmBLayVpNg4ePMiqVas5ceJ4rf1cWEmS\n9Er66NcHqmNsbKwEAhdWkiR1hqFg4LmwkiSpM3xKoiRJAgwFkiSpMBRIkiTAUCBJkgonGnbJyMhI\nrf4vvvgi5513XteOL0nSqzEUdNyzwDw2bNhQc7/XAD/sQj2SJM2OoaDjXgBOUW/9gPuAD7e4jyRJ\nnWEo6Jo66wdM3ApoZR9JkjrDiYaSJAkwFEiSpMJQIEmSAEOBJEkqDAWSJAnw1weahYMHDzI2NlZr\nn6VLl/p4ZkkaMIYCzejgwYOsWrWaEyeO19pvwYKF7N07YjCQpAFiKNCMxsbGSiCos7DSCCdObGBs\nbMxQIEkDxFCgWaqzsJIkaRA50VCSJAGOFMxJdZ6w6NMYJWnuMBTMKa0+wVGSNBcYCuaUdp7gKEk6\n2xkK5iSfxihJejlDgQaaCytJUucYCjSwXFhJkjrLUKCB5cJKktRZhgKdBVxYSZI6wcWLJEkS4EiB\n+kjdSYPtLKxUd98XX3yR8847r9Y+rUxobGXi5JmqTdLZz1CgvtDqpMH6Wl3A6TXAD2vtUXdCY+vX\noPu1SZobDAXqC61NGmxlYaV2FnDq7oTG9q6Bky0ltc9QoD7T7YWVHm3jPGdqQmP/1dZsNmk0Gl07\n/tnIa9Yar1tvdS0URMQHgf8ELAeeBH49M/9Pt86n/tOfD176mzN0nkp/XoP6ms0ml1122ZxfKKrO\nnI9Pf/rTrFq16oxdg35eyKufr5um6kooiIhfBj4G3AA8BmwCdkXEmzOz3v+1GkA+eOlsuwbf//73\n5/xCUa3M+RgaGjoj16CfF/Lq5+uml+vWSMEm4PbM/CxARNwI/CvgOmBbl86pvuGDl862a3Dy5Mk5\nv1BU/Tkfm4Abzsg16OeFvPr5uunlOh4KIuIcYAj4nYm2zMyI+DKwttPnUz/zwUtn3zVwoajZX4PF\nzP4DulP6+b9PP183TejGSMFSqt9IjU5rHwVWnab/guoffwI8PstT7Cn/vI/Z/0X6iPu4D/D8GTpP\nP++zv9rjvvtmPY9hdHTij3N3zwMwb948Tp06Nev+Z2qf/fv3l3+b7TU4VPp2/xrUrw3O1H+fdq5b\nP8+z6ReTrtGCThwvMrMTx/n7A0a8HngGWJuZuye1/y7w9sxcO63/rwCf72gRkiTNLddk5l3tHqQb\nIwVjVCupLJvWvgw4cpr+u4BrgAPAiS7UI0nS2WoB8FNUn6Vt6/hIAUBEfAXYnZk3lfcBHAQ+kZkf\n7fgJJUlS27r164OPA5+JiCf4+58kLgQ+06XzSZKkNnUlFGTmzohYCtxKddvgq8CVmfmdbpxPkiS1\nryu3DyRJ0uCZ1+sCJElSfzAUSJIkoA9CQUR8MCL2R8T3I+IrEfFzva6pX0XELRHxWEQcjYjRiPjT\niHhzr+saNBHxoYg4FREf73Ut/Swi3hARn4uIsYg4HhFPRkS/LpfXFyJiXkRsjYhvlmu2LyI297qu\nfhIRl0fEFyPimfLn8F2n6XNrRBwu1/D+iFjZi1r7yUzXLSLmR8TvRsRTEfG90ueOsm5QLT0NBZMe\nnPRbwD+meprirjJJUS93OfBJ4FLgCuAc4C8i4sd6WtUAKaHzBqr/1/QKImIJ1XKJLwJXUq07+xvA\n3/WyrgHwIeA/AB8Afgb4TeA3I2JjT6vqL+dTTT7/APCySW0RcTOwkerP6SXAMarPhXPPZJF9aKbr\nthB4G/DbVJ+l76ZaQfjuuifp6UTDV1jP4NtU6xn44KRXUcLTc1QrRT7c63r6XUS8FngCeD/Vk4f2\nZOZ/7G1V/SkiPkK1Kuk7el3LIImIe4Ajmfm+SW3/Eziemdf2rrL+FBGngKsz84uT2g4DH83M7eX9\nIqpl8n81M3f2ptL+crrrdpo+FwO7gX+UmYdme+yejRRMenDSAxNtWSUUH5w0e0uoEuPzr9ZRANwG\n3JOZD/a6kAHwi8DjEbGz3Koajojre13UAHgUWBcRPw0QERcBlzGxmL9mFBEXAMuZ+rlwlOrDzc+F\neiY+H16os1O3Fi+ajboPTtIkZVRlB/BwZj7d63r6XUS8h2p47eJe1zIgLqQaUfkY8N+ohnE/EREv\nZubnelpZf/sIsAj4RkT8kOqL13/JzC/0tqyBsZzqg+x0nwvLz3w5gykizqP6f/GuzPxenX17GQrU\nnk8Bb6H6FqIZRMQbqQLUFZn5Uq/rGRDzgMcy88Pl/ZMR8VbgRsBQ8Mp+GfgV4D3A01RB9Pci4rBh\nSmdCRMwH/pgqXH2g7v69nGhY98FJKiLi94GrgH+Wmc/2up4BMAT8BDAcES9FxEvAO4CbIuJkGXXR\nVM/y8ufcjgArelDLINkGfCQz/zgzv56Znwe2A7f0uK5BcQQI/FxoyaRA8CbgnXVHCaCHoaB8Y3sC\nWDfRVv5yXkd1X06nUQLBLwG/kJkHe13PgPgy8LNU39ouKq/HgTuBi9JlPU/nEV5+G28V8K0e1DJI\nFlJ92ZnsFH3w8+9BkJn7qT78J38uLKL6xZWfCzOYFAguBNZlZku/FOr17QMfnFRDRHwKaADvAo5F\nxESaHs9MHzv9CjLzGNVQ7o9ExDHgu5k5/duwKtuBRyLiFmAn1V/K1wPvm3Ev3QNsjohDwNeBNVR/\nr/2PnlbVRyLifGAl1YgAwIVlQubzmfltqlt9myNiH3AA2AocooWf151NZrpuVCN7/4vqi8+/Bs6Z\n9PnwfJ3bpj1/9kFEfIDqt7wTD0769cx8vKdF9anyM5TT/Qf7tcz87JmuZ5BFxIPAV/1J4iuLiKuo\nJiutBPYDH8vMP+xtVf2t/MW9lep34j8JHAbuArZm5g96WVu/iIh3AH/Jy/8uuyMzryt9tlCtU7AE\neAj4YGbuO5N19puZrhvV+gT7p22L8v4XMvOvZ32eXocCSZLUH7zPJUmSAEOBJEkqDAWSJAkwFEiS\npMJQIEmSAEOBJEkqDAWSJAkwFEiSpMJQIEmSAEOBJEkqDAWSJAmA/w/PgO3s7Kz1XAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10996eed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D=[ 0.74,  0.20,  0.56,  0.05,  0.67,  0.41,  0.74,  4.63,  0.59,  0.39,\n",
    "    0.71,  0.17,  5.34,  0.33,  0.01,  1.11,  0.60,  0.41,  0.65,  1.97,\n",
    "    0.19,  0.80,  0.04,  0.48,  0.54,  0.59,  0.31,  1.40,  0.63,  0.38,\n",
    "    0.36,  0.02,  0.68,  0.72,  0.84,  0.30,  0.01,  1.37,  0.89,  0.10,\n",
    "    0.21,  0.68,  0.14,  0.10,  0.11,  0.01,  0.09,  0.50,  0.34,  0.30,\n",
    "    1.22, 10.05,  0.19,  0.04,  0.13,  1.53,  2.28,  1.76,  0.03,  0.31,\n",
    "    0.37,  0.50,  0.05,  0.30,  0.53,  0.63,  4.20,  0.86,  0.29,  1.98,\n",
    "    1.27,  0.35,  0.43,  0.35,  0.75,  0.25,  1.15,  1.65,  0.82,  0.37,\n",
    "    2.55,  2.75,  3.06,  0.97,  2.65,  8.97,  0.04,  2.98,  0.36,  0.01,\n",
    "    0.85,  0.90,  0.09,  0.01,  0.82,  2.30,  2.09,  0.29,  0.16,  2.12,\n",
    "    5.28,  0.27,  0.15,  1.02,  0.51,  0.02,  1.72,  1.35,  0.51,  0.27,\n",
    "    1.05,  2.24,  3.93,  0.62,  3.38,  0.56,  0.49,  2.84,  0.27,  0.12,\n",
    "    3.99,  0.16,  0.09,  3.61,  0.54,  0.08,  0.31,  1.38,  0.63,  0.61,\n",
    "    0.21,  0.13,  2.28,  2.61,  4.60,  0.02,  0.34,  0.15,  0.07,  2.44,\n",
    "    0.86,  0.73,  2.01,  0.26,  0.72,  1.56,  0.09,  0.97,  0.24,  0.92,\n",
    "    1.05,  0.71,  1.28,  3.79,  1.32,  0.17,  0.39,  2.82,  0.12,  2.06,\n",
    "    2.04,  0.00,  1.94,  0.27,  0.91,  0.36,  0.92,  5.69,  0.33,  0.69,\n",
    "    1.00,  2.19,  0.01,  0.08,  1.16,  0.31,  0.83,  0.41,  1.27,  0.08,\n",
    "    4.69,  0.65,  0.43,  0.10,  2.92,  0.06,  6.21,  0.90,  0.00,  0.52,\n",
    "    0.65,  0.26,  1.94,  0.37,  0.50,  5.66,  4.24,  0.40,  0.39,  7.89]\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(D,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, the log-likelihood function is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\alpha,\\beta) &= \\log \\prod_{i=1}^N p(x_i;\\alpha,\\beta) = \\sum_{i=1}^N \\log (e^{-\\alpha x_i} + \\beta e^{-\\beta x_i}) - \\log (2)\n",
    "\\end{align*}\n",
    "\n",
    "Unfortunately, it is difficult to extract the parameters $\\alpha,\\beta$ analytically by solving directly the equation $\\nabla \\ell = 0$. Instead, we will analyze the function over a grid of parameters $\\alpha$, $\\beta$. We know a priori that parameters $\\alpha$ and $\\beta$ are in the intervals $[0.4,1.0]$ and $[1.5,4.5]$ respectively.\n",
    "\n",
    "* **Build a grid on this limited domain and evaluate log-likelihood at each point of the grid.**\n",
    "* **Plot the log-likelihood function as a contour plot, and superpose the grid to it.**\n",
    "\n",
    "Highest log-likelihood values (i.e. most probable parameters) should appear in red, and lowest values should be plotted in blue. Two adjacent lines of the contour plot should represent a log-likelihood difference of 1.0. In your code, favor numpy array operations over Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4   0.55  0.7   0.85]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,4) (200,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-23181c3eea7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mR1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,4) (200,) "
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "R1 = numpy.arange(0.4, 1.0, 0.15)\n",
    "R2 = numpy.arange(1.5, 4.5, 0.3)\n",
    "x = D\n",
    "print R1\n",
    "alpha, beta = numpy.meshgrid(R1,R2)\n",
    "l = sum(numpy.log(numpy.exp(-alpha*x) + beta*numpy.exp(-beta*x)) - numpy.log(2), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.4\n",
    "print alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-32929f41bfd1>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-32929f41bfd1>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    res += (np.log(np.exp(-.4*d) + 1.5*np.exp(-1.5*d)) - np.log(2), axis=o0)\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "alpha = numpy.arange(4)\n",
    "beta = numpy.arange(10)\n",
    "print type(alpha)\n",
    "print alpha\n",
    "print beta\n",
    "x = D\n",
    "for alpha in alpha:\n",
    "    for beta in beta:\n",
    "        l = alpha + beta\n",
    "        print l\n",
    "        \n",
    "res = 0\n",
    "for d in D:\n",
    "    res += (np.log(np.exp(-.4*d) + 1.5*np.exp(-1.5*d)) - np.log(2), axis=o0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradent-Based Optimization (10 P)\n",
    "\n",
    "As an alternative to computing the log-likelihood for a whole grid, we would like to find the optimal parameters $\\alpha,\\beta$ by gradient-based optimization. The partial derivatives of the log-likelihood function are given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell(\\alpha,\\beta)}{\\partial \\alpha} &= \\sum_{i=1}^N \\frac{e^{-\\alpha x_i} (1 - \\alpha x_i)}{\\alpha e^{-\\alpha x_i} + \\beta e^{-\\beta x_i}}\\\\\n",
    "\\frac{\\partial \\ell(\\alpha,\\beta)}{\\partial \\beta} &= \\sum_{i=1}^N \\frac{e^{-\\beta x_i} (1 - \\beta x_i)}{\\alpha e^{-\\alpha x_i} + \\beta e^{-\\beta x_i}}\n",
    "\\end{align*}\n",
    "\n",
    "A gradient ascent step of the log-likelihood function takes the form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\alpha\\\\\n",
    "\\beta\n",
    "\\end{pmatrix} \\leftarrow\n",
    "\\begin{pmatrix}\n",
    "\\alpha\\\\\n",
    "\\beta\n",
    "\\end{pmatrix} + \\gamma \\nabla_{\\alpha,\\beta} \\ell(\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is a learning rate to be defined. We start with initial parameters $\\alpha=0.7$ and $\\beta=3.0$.\n",
    "\n",
    "\n",
    "* **Implement the gradient ascent procedure.**\n",
    "* **Run the gradient ascent with parameter $\\gamma = 0.005$.**\n",
    "* **Plot the trajectory of the gradient ascent in superposition to the contour plot of the previous exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-636202c74bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#alpha = sum((numpy.exp(-alpha*x)*(1 - alpha*x))/(alpha*numpy.exp(-alpha*x) + beta*numpy.exp(-beta*x)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#dbeta = (b*(1 - beta*sum(x)))/(alpha*a + beta*b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#print dalpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "alpha = \n",
    "beta = 1\n",
    "x = D\n",
    "\n",
    "print type(D)\n",
    "#alpha = sum((numpy.exp(-alpha*x)*(1 - alpha*x))/(alpha*numpy.exp(-alpha*x) + beta*numpy.exp(-beta*x)))\n",
    "l = sum(np.log(np.exp(-alpha*x) + beta*np.exp(-beta*x))) - np.log(2)\n",
    "#dbeta = (b*(1 - beta*sum(x)))/(alpha*a + beta*b)\n",
    "#print dalpha\n",
    "#print dbeta\n",
    "print l\n",
    "eta = 0.005\n",
    "alpha = alpha + eta*dalpha\n",
    "beta = beta * eta*dbeta\n",
    "print alpha\n",
    "print beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rdc/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#theta = theta - (alpha/m)*X'*(X*theta-y);\n",
    "\n",
    "\n",
    "a = sum(numpy.exp(-alpha*x))\n",
    "print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the optimization procedure does not converge in reasonable time and seems to oscillate.\n",
    "\n",
    "* **Explain the problem(s) with this approach. Propose a simple improvement of the optimization technique and apply it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[REPLACE BY YOUR EXPLANATION + PROPOSITION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### REPLACE BY YOUR CODE\n",
    "import solution\n",
    "solution.s2c(D)\n",
    "###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
