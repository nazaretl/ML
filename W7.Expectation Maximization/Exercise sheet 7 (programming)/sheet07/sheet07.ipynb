{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:cb971d5d3dd17ae9f085d2baba8b0abc115c5c213dae851dc9a5cd2e21307155"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Expectation-Maximization\n",
      "\n",
      "In this assignment we will be using the Expectation Maximization method to estimate the parameters of the same three coin experiment as in the theoretical part. We will examine the behavior of the algorithm for various combinations of parameters.\n",
      "\n",
      "## Description of the Experiment\n",
      "\n",
      "The following procedure generates the data for the three coin experiment.\n",
      "\n",
      "The parameters are:\n",
      "\n",
      "- $\\lambda$ := The probability of heads on the hidden coin H.\n",
      "\n",
      "- $p_1$ := The probability of heads on coin A.\n",
      "\n",
      "- $p_2$ := The probability of heads on coin B.\n",
      "\n",
      "Each of the $N$ samples is collected the following way:\n",
      "\n",
      "- The secret coin (H) is tossed.\n",
      "\n",
      "- If the result is heads, coin A is tossed $M$ times and the results are recorded.\n",
      "\n",
      "- If the result is tails, coin B is tossed $M$ times and the results are recorded.\n",
      "\n",
      "**Heads are recorded as 1.** \n",
      "\n",
      "**Tails are recorded as 0.**\n",
      "\n",
      "The data is returned as an **$N \\times M$** matrix, where each of the $N$ rows correspond to the trials and contains the results of the corresponding sample (generated either by coin A or by coin B).\n",
      "\n",
      "## Description of Provided Functions\n",
      "\n",
      "Three functions are provided for your convenience:\n",
      "\n",
      "*  **`utils.generateData(lambda,p1,p2,N,M)`:** Performs the experiment $N$ times with coin parameters specified as argument and returns the results in a $N \\times M$ matrix.\n",
      "\n",
      "\n",
      "* **`utils.unknownData()`** Returns a dataset of size $N \\times M$ where generation parameters are unknown.\n",
      "\n",
      "\n",
      "*  **`utils.plot(data,distribution)`:** Plot a histogram of the number of heads per trial along with the probability distribution. This function will be used to visualize the progress of the EM algorithm at every iteration.\n",
      "\n",
      "An example of use of these two functions is given below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy,utils\n",
      "\n",
      "# Print the data matrix as a result of the three coins experiment with parameter 0.5, 0.8 and 0.2.\n",
      "data = utils.generateData(0.5,0.8,0.2,15,5)\n",
      "print data\n",
      "\n",
      "# Print the data histogram along with a uniform probability distribution.\n",
      "utils.plot(data,numpy.ones([data.shape[1]+1])/(data.shape[1]+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Calculate the Log-Likelihood (10 P)\n",
      "\n",
      "Implement a function which calculates the log likelihood for a given dataset and parameters. The log-likelihood is given by:\n",
      "$$\n",
      "LL = \\frac1N \\sum_{i=1}^N \\log \\!\\! \\sum_{z \\in \\{\\mathrm{heads},\\mathrm{tails}\\}} \\!\\! P(X=x_i, Z=z \\mid \\theta)\\\\\n",
      "   = \\frac1N \\sum_{i=1}^N \\log \\left[ \\lambda \\cdot p_1^{h(x_i)} \\cdot (1-p_1)^{t(x_i)} + (1-\\lambda) \\cdot p_2^{h(x_i)} \\cdot (1-p_2)^{t(x_i)} \\right]\n",
      "$$\n",
      "where $h(x_i)$ and $t(x_i)$ denote the number of heads and tails in sample $i$, respectively. Note that we take the averaged log-likelihood over all trials, hence the multiplicative term $\\frac1N$ in front."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loglikelihood(data,lam,p1,p2):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementing and Running the EM Algorithm (30 P)\n",
      "\n",
      "Implement a function which iteratively determines the values of $\\lambda$, $p_1$ and $p_2$. The function starts with some initial estimates for the parameters and returns the results of the method for those parameters.\n",
      "\n",
      "In each iteration, the following update rules are used for the parameters:\n",
      "\n",
      "$$\\lambda^{new} = \\frac{E(\\#heads(coin\\_H))}{\\#throws(coin\\_H)} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\lambda p_1^{h(x_i)}(1-p_1)^{t(x_i)}}{\\lambda p_1^{h(x_i)}(1-p_1)^{t(x_i)} + (1-\\lambda)p_2^{h(x_i)}(1-p_2)^{t(x_i)}}$$\n",
      "\n",
      "$$p_1^{new} = \\frac{E(\\#heads(coin\\_A))}{E(\\#throws(coin\\_A))} = \\frac{\\sum_{i=1}^{N}R_1(i)h(x_i)}{M \\sum_{i=1}^{N}R_1(i)}$$\n",
      "\n",
      "$$p_2^{new} = \\frac{E(\\#heads(coin\\_B))}{E(\\#throws(coin\\_B))} = \\frac{\\sum_{i=1}^{N}R_2(i)h(x_i)}{M \\sum_{i=1}^{N}R_2(i)}$$\n",
      "\n",
      "where $h(x_i)$ and $t(x_i)$ denote the number of heads and tails in sample i, respectively, and\n",
      "\n",
      "$$R_1(i) = \\frac{\\lambda p_1^{h(x_i)}(1-p_1)^{t(x_i)}}{\\lambda p_1^{h(x_i)}(1-p_1)^{t(x_i)} + (1-\\lambda)p_2^{h(x_i)}(1-p_2)^{t(x_i)}}$$\n",
      "\n",
      "$$R_2(i) = \\frac{(1-\\lambda) p_2^{h(x_i)}(1-p_2)^{t(x_i)}}{\\lambda p_1^{h(x_i)}(1-p_1)^{t(x_i)} + (1-\\lambda)p_2^{h(x_i)}(1-p_2)^{t(x_i)}}$$\n",
      "\n",
      "\n",
      "**TODO:**\n",
      "\n",
      "* **Implement the EM learning procedure.**\n",
      "* **Use as stopping criterion the improvement of log-likelihood between two iterations to be smaller than $0.001$.**\n",
      "* **Run the EM procedure on the data returned by function `utils.unknownData()`. Use as an initial solution for your model the parameters $\\lambda = 0.5$, $p_1 = 0.25$, $p_2 = 0.75$ **.\n",
      "* **At each iteration of the EM procedure, print the log-likelihood and the value of your model parameters, and plot the learned probability distribution using the function utils.plot().**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import utils\n",
      "%matplotlib inline\n",
      "\n",
      "# --------------------\n",
      "# REPLACE BY YOUR CODE\n",
      "import solution; solution.EM()\n",
      "# --------------------\n",
      "\n",
      "\n",
      "# -------------------------------------------\n",
      "# Template for your code\n",
      "# -------------------------------------------\n",
      "#TODO: Initialize the model\n",
      "lam = None\n",
      "p1  = None\n",
      "p2  = None\n",
      "\n",
      "criterion = None # (to be set to True)\n",
      "\n",
      "# Iterate until the stopping criterion is satisfied\n",
      "while (criterion == False):\n",
      "\n",
      "    #TODO:\n",
      "    # - Perform one step of EM\n",
      "    # - Print the log-likelihood and the model parameters\n",
      "    # - Plot data histogram and the learned probability distribution\n",
      "    # - Determine if stopping criterion is satisfied\n",
      "\n",
      "    print 'it:%2d  lambda: %.2f  p1: %.2f  p2: %.2f  log-likelihood: %.3f'%(\n",
      "            it, lam, p1, p2, loglikelihood)\n",
      "# -------------------------------------------"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## More Experiments (10 P)\n",
      "\n",
      "Examine the behaviour of the EM algorithm for various combinations of data generation parameters and initializations (for generating various distributions, use the method `utils.generateData(...)`). In particular, find settings for which:\n",
      "\n",
      "* The role of coins $A$ and $B$ are permuted between the data generating model and the learned model (i.e. $\\hat p_1 \\approx p_2$, $\\hat p_2 \\approx p_1$ and $\\hat \\lambda \\approx 1-\\lambda$).\n",
      "\n",
      "* The EM procedure takes a long time to converge.\n",
      "\n",
      "Print the parameters and log-likelihood objective at each iteration. Only display the plot for the converged model."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}